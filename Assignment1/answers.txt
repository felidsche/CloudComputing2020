CPU BENCHMARK QUESTIONS
1. Sysbench performs CPU benchmarking via verifying prime numbers using standard division. For this, the number is divided by all other numbers between 2 and its square root. If the remainder is zero, the next number is verified.
The events/s represent events per second, meaning calculation operations on the CPU per second. Thus, the time for shared memory access of several threads, etc. is not measured. If several threads are used, this is also taken into account.

2. Both providers showed partly enormous drops or peaks of the CPU performance. To examine this behavior scientifically sound, the period of measurement of two days is too short. Therefore, only hypotheses can be given. E.g., one reason, especially for the AWS could, be the system of AWS's EC2 CPU Credits for t2 instances. Using these credits, a t2 instance can increase its CPU performance beyond the basic limit. The credits are earned over time, thus accumulated, depending on the instance type. This could potentially explain the peaks on Monday.
The CPU performance drops of GCP between Wednesday and Thursday occur especially in the night hours, which could be a hint that there are higher resource demands on the cloud (i.e.due to nightly batch processing). This results in a slightly higher access time until our processes gain the computation resources.
However, to validate these hypotheses, long-term measurements are inevitable.

MEMORY BENCHMARK QUESTIONS
1. Performing memory benchmark tests, sysbench allocates a memory buffer and reads or writes from or into this buffer for the size of a pointer (32 or 64 bit). This is executed until the total memory size has been read or written.

2. In general, we would expect that virtualization decreases the memory benchmark, since in some cases the physical memory is not located at the same place as the virtual machine, or, as in most cases, at least a hypervisor has to maintain a shadow page table or similar, thus increasing the time for memory access. 
Additionally, in full virtualization, every resource is just virtual, so every benchmark is subject to fluctuation of the actual currently available resources, network fluctuations such as the round trip time, performance limitations e.g. due to missing credits or set limitations of the hypervisor in case of higher priority for other processes, and similar. 
Thus, memory benchmarking would be subject to high fluctuations in time.

DISK BENCHMARK QUESTIONS
1. At first, sysbench stores the specified amount of files to the disk. The used storage is specified by the total size. The file is then read by the specified mode (random or sequential) or the files are rewritten block-wise in case of write mode.

2. The main difference is that sequential writes are stored, as the name hints, sequential in the memory, so the blocks can be read sequentially. In random read/write mode, this is not the case. This leads to (often, not every time) better memory usage, but decreases performance. 
The main difference between AWS and GCP is, that the instance type of AWS uses an SSD, while GCP uses an HDD. Therefore, in random read mode, where an SSD doesn't have to move the read head to the actual block position due to the flash technology, AWS is much faster. Nevertheless, for sequential read mode, GCP's instance was faster, although it uses an HDD, which is typically slower than an SSD. 
This might be because GCP optimized its persistent disks for sequential reads. Furthermore, using random read mode, the performance of GCP is constant, which might signalise that for random reads GCP uses a local volume, etc., which is not subject to fluctuations in the network or the place of the physical disk.

GENERAL QUESTIONS
1. Overall, in most cases, the VM of GCP seems to perform better than the one of AWS. Only the random read of disks was much better in AWS, but this is a quite important fact, since when small files or chunks of data are accessed and to be stored, AWS is the provider to be chosen, since the random storage is more efficient for small amounts of data. 
For example, some web services, where customers or users can access a lot of different small files (e.g. services with a lot of pictures), might perform better on AWS, since these small files are better to be stored randomly and thus AWS should be faster accessing them. 
For other use cases, especially ones with sequential read and writes of big files or computational more heavy tasks, such as machine learning, we would suggest GCP as a provider.